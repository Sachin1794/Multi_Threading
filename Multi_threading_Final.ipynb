{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_connURLsplit(connURL):\n",
    "    return connURL.split('/')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_MSSQL(credentials, connType):\n",
    "    serverName = credentials[0]\n",
    "    userName = credentials[1]\n",
    "    password = credentials[2]\n",
    "    databaseName = credentials[3]\n",
    "\n",
    "    if connType == 'SQLServer':\n",
    "        # Configure connection properties for SQL Server\n",
    "        jdbcUrl = f\"jdbc:sqlserver://{serverName};database={databaseName}\"\n",
    "        connectionProperties ={\n",
    "            \"user\": userName,\n",
    "            \"password\": password,\n",
    "            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "        }\n",
    "    elif connType == 'MySQL':\n",
    "        # Configure connection properties for MySQL\n",
    "        jdbcUrl = f\"jdbc:mysql://{serverName}:3306/{databaseName}?user={userName}&password={password}\"\n",
    "        connectionProperties = {\n",
    "            \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Connection type not recognized\")\n",
    "\n",
    "    credentialsProcessed = [serverName, userName, password, databaseName, jdbcUrl, connectionProperties]\n",
    "    return credentialsProcessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def udf_copyDataFileToMSSQL(pipelineUUID,conf_Initial,sourceTable,targetTable,srcCredentials,tgtCredentials,srcConnType,tgtConnType,loadType='full'): # change to incremental\n",
    "\n",
    "    try:\n",
    "        # print(tgtConnType)\n",
    "        execStartTimestamp = str(datetime.datetime.now())[:25]\n",
    "        tableUUID = str(uuid.uuid4())\n",
    "        tableName = os.path.basename(sourceTable).split('.')[0]\n",
    "        tableStatus = 'Completed'\n",
    "        tableMessage = ''\n",
    "        srcCnt = ''\n",
    "        tgtCnt = ''\n",
    "\n",
    "  \n",
    "        if srcConnType == 'MySQL':\n",
    "            src = readData(sourceTable=sourceTable, srcCredentials=srcCredentials, srcConnType=srcConnType, loadType='full')\n",
    "            srcCnt = src.count()\n",
    "            if len(targetTable) == 0: targetTable = 'stg.'+os.path.basename(sourceTable).split('.')[0]\n",
    "\n",
    "\n",
    "        elif (srcConnType=='File'):\n",
    "            src = readData(sourceTable=sourceTable, srcCredentials=srcCredentials, srcConnType=srcConnType, loadType='full')\n",
    "            srcCnt = src.count()\n",
    "            if len(targetTable) == 0: targetTable = 'stg.'+os.path.basename(sourceTable).split('.')[0]\n",
    "\n",
    "        elif (srcConnType=='SQLServer'):\n",
    "            src = readData(sourceTable=sourceTable, srcCredentials=srcCredentials, srcConnType=srcConnType, loadType='full')\n",
    "            srcCnt = src.count()\n",
    "            if len(targetTable) == 0: targetTable='C:\\\\Users\\\\sachin.gupta\\\\Desktop\\\\Threading Test Folder SRC\\\\'+sourceTable\n",
    "\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print('srcConnType is not defined')\n",
    "            \n",
    "  \n",
    "        \n",
    "        if tgtConnType == 'SQLServer':\n",
    "            \n",
    "            tgtCnt = writeData(src, targetTable=targetTable, tgtCredentials=tgtCredentials, tgtConnType=tgtConnType)\n",
    "\n",
    "        elif tgtConnType=='File':\n",
    "            tgtCnt = writeData(src, targetTable=targetTable, tgtCredentials=tgtCredentials, tgtConnType=tgtConnType)\n",
    "\n",
    "\n",
    "            \n",
    "        execEndTimestamp = str(datetime.datetime.now())[:25]\n",
    "        values =[pipelineUUID, tableUUID, tableName, execStartTimestamp, execEndTimestamp, tableStatus, tableMessage, srcCnt, tgtCnt]\n",
    "        udf_insertLogs(conf_Initial[0:4],'table',values)    \n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        execEndTimestamp = str(datetime.datetime.now())[:25]\n",
    "        tableStatus = 'Failed'\n",
    "        tableMessage = str(e)[:128]\n",
    "        values =[pipelineUUID, tableUUID, tableName, execStartTimestamp, execEndTimestamp, tableStatus, tableMessage, srcCnt, tgtCnt]\n",
    "        udf_insertLogs(conf_Initial[0:4],'table',values)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def copy_files_with_threads(functionName,pipelineUUID,conf_Initial,srcCredentials='',tgtCredentials='',srcTablesList='',tgtTablesList='',srcConnType='',tgtConnType=''):# will take full df here as input with cols: plName,srcTable,tgtTable\n",
    "\n",
    "    threads = []\n",
    "   \n",
    "    for srcDataFile in srcTablesList:\n",
    "        # target_file_name = 'dbo.'+os.path.basename(source_path).split('.')[0]\n",
    "        # destination_path = os.path.join(destination_folder, file_name)\n",
    "        tgtTbl = '' # THIS NEEDS TO BE CHANGED AS PER THE TARGET FUNCTIONALITY\n",
    "        # udf_copyDataFileToMSSQL(pipelineUUID,conf_Initial,sourceTable,targetTable,srcCredentials,tgtCredentials,srcConnType,tgtConnType='tgt',loadType='full')\n",
    "        thread = Thread(target=functionName, args=(pipelineUUID,conf_Initial,srcDataFile,tgtTbl,srcCredentials,tgtCredentials,srcConnType,tgtConnType))\n",
    "        # print(tgtConnType)\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyData(pipelineUUID, conf_Initial, srcCredentials, tgtCredentials, srcTablesList, tgtTablesList, srcConnType, tgtConnType):\n",
    "    if (srcConnType == 'File' and tgtConnType == 'SQLServer'):\n",
    "        copy_files_with_threads(functionName=udf_copyDataFileToMSSQL,\n",
    "                                pipelineUUID=pipelineUUID,\n",
    "                                conf_Initial=conf_Initial,\n",
    "                                srcCredentials=srcCredentials,\n",
    "                                tgtCredentials=tgtCredentials,\n",
    "                                srcTablesList=srcTablesList,\n",
    "                                tgtTablesList=tgtTablesList,\n",
    "                                srcConnType=srcConnType,\n",
    "                                tgtConnType=tgtConnType)\n",
    "    elif( srcConnType == 'MySQL' and tgtConnType == 'SQLServer'):\n",
    "        copy_files_with_threads(functionName=udf_copyDataFileToMSSQL,\n",
    "                                pipelineUUID=pipelineUUID,\n",
    "                                conf_Initial=conf_Initial,\n",
    "                                srcCredentials=srcCredentials,\n",
    "                                tgtCredentials=tgtCredentials,\n",
    "                                srcTablesList=srcTablesList,\n",
    "                                tgtTablesList=tgtTablesList,\n",
    "                                srcConnType=srcConnType,\n",
    "                                tgtConnType=tgtConnType)\n",
    "\n",
    "    elif srcConnType == 'SQLServer' and tgtConnType == 'File':\n",
    "        copy_files_with_threads(functionName=udf_copyDataFileToMSSQL,\n",
    "                                pipelineUUID=pipelineUUID,\n",
    "                                conf_Initial=conf_Initial,\n",
    "                                srcCredentials=srcCredentials,\n",
    "                                tgtCredentials=tgtCredentials,\n",
    "                                srcTablesList=srcTablesList,\n",
    "                                tgtTablesList=tgtTablesList,\n",
    "                                srcConnType=srcConnType,\n",
    "                                tgtConnType=tgtConnType)\n",
    "        \n",
    "    \n",
    "\n",
    "    else:\n",
    "        print('Src-Tgt connection combination not recognized')\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(sourceTable='', srcCredentials='', srcConnType='', loadType='full'):\n",
    "\n",
    "\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CSV to SQL Server\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", path_sqljdbc_jar).getOrCreate()\n",
    "\n",
    "    if srcConnType == 'File':\n",
    "        df = spark.read.csv(sourceTable, header=True, inferSchema=True)\n",
    "    elif srcConnType == 'SQLServer':\n",
    "        df = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", srcCredentials[4]) \\\n",
    "            .option(\"dbtable\", sourceTable) \\\n",
    "            .option(\"user\", srcCredentials[1]) \\\n",
    "            .option(\"password\", srcCredentials[2]) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "    elif srcConnType == 'MySQL':\n",
    "        path_MYsqljdbc_jar = \"C:\\\\Users\\\\sachin.gupta\\\\Downloads\\\\mysql-connector-j-8.1.0.jar\"\n",
    "\n",
    "        \n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"CSV to SQL Server\") \\\n",
    "            .config(\"spark.driver.extraClassPath\", path_MYsqljdbc_jar).getOrCreate()\n",
    "\n",
    "        jdbc_url=f\"jdbc:mysql://{serverName}/{databaseName}\"\n",
    "        df = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", sourceTable) \\\n",
    "            .option(\"user\", userName) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "            .load()\n",
    "\n",
    "   \n",
    "    else:\n",
    "        raise ValueError(\"Source Connection Type not recognized\")\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeData(df,targetTable,tgtCredentials,tgtConnType):\n",
    "\n",
    "    if tgtConnType=='SQLServer':\n",
    "        \n",
    "        tgtServerName, tgtUserName, tgtPassword, tgtDatabaseName, tgtJdbcUrl, tgtConnectionProperties = tgtCredentials\n",
    "\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"CSV to SQL Server\") \\\n",
    "            .config(\"spark.driver.extraClassPath\", path_sqljdbc_jar).getOrCreate()\n",
    "\n",
    "        # schema_query=f\"Select column_name from information_schema.columns where Table_Name = %s\"\n",
    "        # params = (TableName)\n",
    "        # cursor.execute(schema_query,params)\n",
    "        \n",
    "        schemaName,tableName = targetTable.split('.')\n",
    "       \n",
    "        \n",
    "        metadata_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", tgtJdbcUrl) \\\n",
    "        .option(\"dbtable\", f\"(SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{tableName}') AS t\") \\\n",
    "        .option(\"user\", tgtUserName) \\\n",
    "        .option(\"password\", tgtPassword) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .load()\n",
    "       \n",
    "       \n",
    "        tableName='['+tableName+']'\n",
    "\n",
    "        table_exists = metadata_df.count() > 0\n",
    "       \n",
    "        # table_exists = spark.catalog.tableExists(TableName)\n",
    "        if table_exists:\n",
    "            df.write.jdbc(url=tgtJdbcUrl, table=schemaName+'.'+tableName, mode=\"append\", properties=tgtConnectionProperties)\n",
    "        else:\n",
    "            df.write.jdbc(url=tgtJdbcUrl, table=schemaName+'.'+tableName, mode=\"overwrite\", properties=tgtConnectionProperties)\n",
    "\n",
    "        tgtCnt = df.count()\n",
    "\n",
    "    elif tgtConnType=='File':\n",
    "        df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(targetTable)\n",
    "\n",
    "        \n",
    "        # spark.stop()\n",
    "        \n",
    "    return tgtCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confInitial():\n",
    "\n",
    "    ServerName = 'psslsql-1.database.windows.net'\n",
    "    UserName = 'sqladmin'\n",
    "    Password = 'X*33aMoff92m'\n",
    "    DatabaseName = 'demodb'\n",
    "\n",
    "    path_sqljdbc_jar = \"C:\\\\Users\\\\sachin.gupta\\\\Desktop\\\\sqljdbc42-6.0.8112.jar\"\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CSV to SQL Server\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", path_sqljdbc_jar).getOrCreate()\n",
    "\n",
    "    jdbc_url = f\"jdbc:sqlserver://{ServerName};database={DatabaseName}\"\n",
    "\n",
    "    pipelines_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"cfg.pipelines\") \\\n",
    "    .option(\"user\", UserName) \\\n",
    "    .option(\"password\", Password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "\n",
    "    connections_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", f\"(SELECT \\\n",
    "        pipelineKey, \\\n",
    "        src.connType as srcConnType,src.connURL as srcConnURL, \\\n",
    "        tgt.connType as tgtConnType,tgt.connURL as tgtConnURL  \\\n",
    "        FROM cfg.pipelines p \\\n",
    "        JOIN cfg.connections src ON srcConn=src.connKey  \\\n",
    "        JOIN cfg.connections tgt ON tgtConn=tgt.connKey) as tbl\") \\\n",
    "    .option(\"user\", UserName) \\\n",
    "    .option(\"password\", Password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    "\n",
    "    initialConf = [ServerName,UserName,Password,DatabaseName,path_sqljdbc_jar,spark,jdbc_url,pipelines_df,connections_df]\n",
    "\n",
    "    return initialConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connReader(connType='', connURL='', tables_df='', connReaderType=''):\n",
    "    credentials = ''\n",
    "    tablesList = ''\n",
    "\n",
    "    if connReaderType == 'src':\n",
    "        if connType == 'File':\n",
    "            # Add srcConnURL to tables_df.tblName\n",
    "            tables_df = tables_df.withColumn('fullFilePath', sf.concat(lit(connURL), lit('\\\\'), col(\"tableName\")))\n",
    "            tablesList = list(tables_df.select(tables_df.fullFilePath).toPandas()['fullFilePath'])\n",
    "            srcConnDetails = [credentials, tablesList]\n",
    "            return srcConnDetails\n",
    "        elif connType == 'SQLServer':\n",
    "            credentialsList = udf_connURLsplit(connURL)\n",
    "            credentials = conf_MSSQL(credentialsList, connType)\n",
    "            tablesList = list(tables_df.select(tables_df.tableName).toPandas()['tableName'])\n",
    "            srcConnDetails = [credentials, tablesList]\n",
    "            return srcConnDetails\n",
    "        elif connType == 'MySQL':\n",
    "            # For MySQL source connection, credentials will be same as srcConnURL\n",
    "            credentialsList = udf_connURLsplit(connURL)\n",
    "            credentials = conf_MSSQL(credentialsList, connType)\n",
    "            tablesList = list(tables_df.select(tables_df.tableName).toPandas()['tableName'])\n",
    "            srcConnDetails = [credentials, tablesList]\n",
    "            return srcConnDetails\n",
    "        else:\n",
    "            print('Source Connection Details incorrect or not working')\n",
    "            return None\n",
    "    elif connReaderType == 'tgt':\n",
    "        if connType == 'SQLServer':\n",
    "            credentialsList = udf_connURLsplit(connURL)\n",
    "            credentials = conf_MSSQL(credentialsList, connType)\n",
    "            tgtConnDetails = [credentials, tablesList]\n",
    "            return tgtConnDetails\n",
    "        elif connType== 'File':\n",
    "            tables_df = tables_df.withColumn('fullFilePath', sf.concat(lit(connURL), lit('\\\\'), col(\"tableName\")))\n",
    "            tablesList = list(tables_df.select(tables_df.fullFilePath).toPandas()['fullFilePath'])\n",
    "            tgtConnDetails = [credentials, tablesList]\n",
    "            return tgtConnDetails\n",
    "        else:\n",
    "            print('Target Connection Details incorrect or not working')\n",
    "            return None\n",
    "    else:\n",
    "        print('Connection Type is Invalid')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_insertLogs(conf_Initial, type, values):\n",
    "\n",
    "   \n",
    "    ServerName,UserName,Password,DatabaseName = conf_Initial\n",
    "    conn = pymssql.connect(server=ServerName, user=UserName, password=Password, database=DatabaseName)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    if type=='pipeline':\n",
    "        tableName = 'logs.exec_pipelines'\n",
    "    if type=='table':\n",
    "        tableName = 'logs.exec_tables'\n",
    "    if type=='error':\n",
    "        tableName = 'logs.exec_errors'\n",
    "\n",
    "    # var_string = ', '.join('?' * len(values))\n",
    "    \n",
    "    insertLogQuery = f\"INSERT INTO {tableName} VALUES  %r\"% (tuple(values),)\n",
    " \n",
    "    # json.dumps\n",
    "    cursor.execute(insertLogQuery)\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit\n",
    "from pyspark.sql import functions as sf\n",
    "import uuid\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import pymssql\n",
    "\n",
    "import shutil\n",
    "import threading\n",
    "import time\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "from pyspark.sql.functions import col,lit\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "\n",
    "conf_Initial = confInitial()\n",
    "ServerName,UserName,Password,DatabaseName,path_sqljdbc_jar,spark,jdbc_url,pipelines_df,connections_df = conf_Initial\n",
    "\n",
    "for pipelineKey in list(pipelines_df.select(pipelines_df.pipelineKey).toPandas()['pipelineKey']):\n",
    "    \n",
    "    execStartTimestamp = str(datetime.datetime.now())[:25]\n",
    "    pipelineDetails = pipelines_df.where(col(\"pipelineKey\")==pipelineKey)\n",
    "    pipelineUUID = str(uuid.uuid4())\n",
    "    pipelineName = list(pipelineDetails.select(pipelineDetails.pipelineName).toPandas()['pipelineName'])[0]\n",
    "    pipelineStatus = 'Succeeded'\n",
    "    pipelineMessage = ''\n",
    "    \n",
    "    try:\n",
    "\n",
    "        tables_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"cfg.tables\") \\\n",
    "        .option(\"user\", UserName) \\\n",
    "        .option(\"password\", Password) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .load()\n",
    "\n",
    "        tables_df = tables_df.where(col(\"pipelineKey\")==pipelineKey).where(col(\"loadType\")=='full')\n",
    "        # .where(col(\"tblName\")=='Dimensions.csv')\n",
    "        # THIS (\"loadType\")=='full' PART needs to be removed after incremental code addition\n",
    "        conn = connections_df.where(col(\"pipelineKey\")==pipelineKey)\n",
    "        \n",
    "        srcConnType = list(conn.select(conn.srcConnType).toPandas()['srcConnType'])[0]\n",
    "        srcConnURL = list(conn.select(conn.srcConnURL).toPandas()['srcConnURL'])[0]\n",
    "        tgtConnType = list(conn.select(conn.tgtConnType).toPandas()['tgtConnType'])[0]\n",
    "        tgtConnURL = list(conn.select(conn.tgtConnURL).toPandas()['tgtConnURL'])[0]\n",
    "\n",
    "        srcConnDetails = connReader(srcConnType, srcConnURL, tables_df, 'src')\n",
    "        tgtConnDetails = connReader(tgtConnType, tgtConnURL, tables_df, 'tgt')\n",
    "        # srcConnDetails can be--> filesList,credentailsList\n",
    "        # tgtConnDetails can be--> only credentailsList\n",
    "\n",
    "        srcCredentials, srcTablesList = srcConnDetails\n",
    "        tgtCredentials, tgtTablesList = tgtConnDetails\n",
    "\n",
    "   \n",
    "        copyData(pipelineUUID,conf_Initial,srcCredentials,tgtCredentials,srcTablesList,tgtTablesList,srcConnType,tgtConnType)\n",
    "\n",
    "        exec_tables = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", \"[logs].[exec_tables]\") \\\n",
    "        .option(\"user\", UserName) \\\n",
    "        .option(\"password\", Password) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .load()\n",
    "\n",
    "        exec_tables = exec_tables.where(col(\"pipelineUUID\")==pipelineUUID)\n",
    "       \n",
    "        \n",
    "\n",
    "        has_failed = exec_tables.filter(col('tableStatus') == 'Failed').count() > 0\n",
    "\n",
    "        pipelineStatus = 'Failed' if has_failed else pipelineStatus\n",
    "\n",
    "        if pipelineStatus == 'Failed':\n",
    "                # Raise a custom exception to force the code to enter the except block\n",
    "            raise Exception(\"Pipeline Failed\")\n",
    "        \n",
    "       \n",
    "        execEndTimestamp = str(datetime.datetime.now())[:25]\n",
    "        values = [pipelineUUID,\tpipelineName, execStartTimestamp, execEndTimestamp,\tpipelineStatus,\tpipelineMessage]\n",
    "      \n",
    "        udf_insertLogs(conf_Initial[0:4],'pipeline',values)\n",
    "        \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        execEndTimestamp = str(datetime.datetime.now())[:25]\n",
    "        pipelineStatus = 'Failed'\n",
    "        pipelineMessage = str(e)\n",
    "        values = [pipelineUUID, pipelineName, execStartTimestamp, execEndTimestamp, pipelineStatus, pipelineMessage]\n",
    "        udf_insertLogs(conf_Initial[0:4],'pipeline',values)\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "    # def copyData(srcConnType,srcConnDetails,tgtConnType,tgtConnDetails):\n",
    "\n",
    "    #     if(srcConnType=='File' and tgtConnType=='SQLServer'):\n",
    "    #         # tgtCredentials = conf_MSSQL(tgtConnDetails)\n",
    "    #         copy_files_with_threads(srcList=srcConnDetails,srcConnType=srcConnType,functionName=udf_copyDataFileToMSSQL,tgtCredentials=tgtConnDetails,tgtConnType=tgtConnType)\n",
    "    #     else:\n",
    "    #         print('Src-Tgt connection type not recognised')\n",
    "\n",
    "    # ,srcConnType,srcConnDetails,tgtConnType,tgtConnDetails\n",
    "    # copy_files_with_threads(functionName=copyData,srcCredentials=srcCredentials,tgtCredentials=tgtCredentials,\n",
    "    #                         srcTablesList=srcTablesList,tgtTablesList=tgtTablesList,srcConnType=srcConnType,tgtConnType=tgtConnType)\n",
    "    # copyData(srcConnType,srcConnDetails,tgtConnType,tgtConnDetails)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
